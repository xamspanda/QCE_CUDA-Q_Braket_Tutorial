{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24d487a9-b18d-4917-856d-bc5bcead84f0",
   "metadata": {},
   "source": [
    "# Lab 1 - Classical QMC with AWS Batch\n",
    "\n",
    "In this lab, we will run a classical QMC workload on [AWS Batch](https://docs.aws.amazon.com/batch/latest/userguide/what-is-batch.html). AWS Batch helps you to run batch computing workloads on the AWS Cloud and makes it easy for developers, scientists, and engineers to access large amounts of compute resources. AWS Batch removes the undifferentiated heavy lifting of configuring and managing the required infrastructure. \n",
    "\n",
    "As a fully managed service, AWS Batch helps you to run batch computing workloads of any scale. AWS Batch automatically provisions compute resources and optimizes the workload distribution based on the quantity and scale of the workloads. With AWS Batch, there's no need to install or manage batch computing software, so you can focus your time on analyzing results and solving problems.\n",
    "\n",
    "In the following, we will go through the steps to\n",
    "1. Create an AWS Batch compute environment\n",
    "2. Package the QMC code for our workload in a Docker image and upload it to the AWS cloud\n",
    "3. Run a small scale QMC example requiring only one compute instance\n",
    "4. Run a large scale QMC example with 200 child jobs running on multiple compute instances in parallel\n",
    "5. Compare the results of our small and large scale experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566df56a-0f26-46c9-b577-04230092a155",
   "metadata": {},
   "source": [
    "### AWS SDK for Python (Boto3)\n",
    "\n",
    "Troughout the tutorial, we will interact with AWS service APIs with [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html), the AWS SDK for Python. Boto3 simplifies the use of AWS services by providing a set of libraries that are consistent and familiar for Python developers. The AWS SDK for Python provides object-oriented APIs for each AWS service we will use in this lab. Let's instantiate the [Boto3 clients](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/clients.html#low-level-clients) for the service APIs we need in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad52a39-ea27-4dd8-9b86-00547957ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "cfn_client = boto3.client('cloudformation')\n",
    "batch_client = boto3.client('batch')\n",
    "sts_client = boto3.client(\"sts\")\n",
    "s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925943ab-553d-404d-9383-8b29104b7de2",
   "metadata": {},
   "source": [
    "We also use Boto3 to get information about the AWS account and AWS region we use in this lab. An AWS region is one component of the [AWS global infrastructure](https://aws.amazon.com/about-aws/global-infrastructure). It is a separate geographic area designed to be isolated from the other regions. When you use a service, select a region to determine in which geographic location your resources will be deployed. When you view your resources, you see only the resources that are tied to the region that you specified. This is because regions are isolated from each other, and we don't automatically replicate resources across regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc4f97c-fe3b-402d-87dc-91c17577c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCOUNT_ID = sts_client.get_caller_identity().get(\"Account\")\n",
    "print(\"Account ID:\", ACCOUNT_ID)\n",
    "\n",
    "my_session = boto3.session.Session()\n",
    "WORKING_REGION = my_session.region_name\n",
    "print(\"Region:\", WORKING_REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50158bb-2baf-4315-8a2f-a44913f65753",
   "metadata": {},
   "source": [
    "### Create the AWS Batch components\n",
    "\n",
    "Batch provides all of the necessary functionality to run high-scale, compute-intensive workloads on top of AWS managed container orchestration services. The service runs your workloads in Docker containers from images you provide and that are pulled from container registries, which may exist within or outside of your AWS infrastructure.\n",
    "\n",
    "Let's create the infrastructure we need to run our experiments today. To use AWS Batch we need the following compontents:\n",
    "* A Batch __[compute environment](https://docs.aws.amazon.com/batch/latest/userguide/compute_environments.html)__ defining the compute resources used to run on jobs on.\n",
    "* A Batch __[job queue](https://docs.aws.amazon.com/batch/latest/userguide/job_queues.html)__ where we submit our jobs to so they can be scheduled to run in a compute environment.\n",
    "* A Batch __[job definition](https://docs.aws.amazon.com/batch/latest/userguide/job_definitions.html)__ specifying the blueprint how jobs are to be run, including memory and CPU requirements as well as container properties like the container image and environment variables.\n",
    "\n",
    "For our tutorial, we will also need\n",
    "* An __[Amazon Simple Storage Service (S3) bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html)__ we use to store job input and output data.\n",
    "* An __[Amazon Elastic Container Registry (ECR) repository](https://docs.aws.amazon.com/AmazonECR/latest/userguide/what-is-ecr.html)__ where we upload and store the Docker image defining our job runtime.\n",
    "\n",
    "![](./images/batch-architecture.png)\n",
    "\n",
    "We can choose to create all these resources through the web browser in the AWS management console, or progammatically using AWS SDKs or the AWS CLI. Today, we will use [AWS CloudFormation](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html), instead. CloudFormation is a service that helps you model and set up your AWS resources so that you can spend less time managing those resources and more time focusing on your applications that run in AWS. We use the template [`batch-environment.yaml`](./batch-environment.yaml) that describes all the AWS resources we need for our QMC simulations with AWS Batch, and CloudFormation takes care of provisioning and configuring those resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96465939-7c8e-4d0e-9f8c-43bdc3cfce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('batch-environment.yaml', 'r') as file:\n",
    "    template_body = file.read()\n",
    "\n",
    "\n",
    "stack_name = 'batch-environment'\n",
    "\n",
    "try:\n",
    "    print(f\"Creating CloudFormation stack {stack_name}\")\n",
    "    cfn_client.create_stack(\n",
    "        StackName=stack_name,\n",
    "        TemplateBody=template_body,\n",
    "    )\n",
    "    print(\"Waiting for CloudFormation stack to complete...\")\n",
    "    waiter = cfn_client.get_waiter(\"stack_create_complete\")\n",
    "    waiter.wait(\n",
    "        StackName=stack_name,\n",
    "        WaiterConfig={\n",
    "            'Delay': 10,\n",
    "            'MaxAttempts': 150\n",
    "        }\n",
    "    )\n",
    "    print(\"CloudFormation stack completed.\")\n",
    "except cfn_client.exceptions.AlreadyExistsException:\n",
    "    print(\"Stack already exists. Updating CloudFormation stack\")\n",
    "    try:\n",
    "        cfn_client.update_stack(\n",
    "            StackName=stack_name,\n",
    "            TemplateBody=template_body,\n",
    "        )\n",
    "        print(\"Waiting for CloudFormation stack to be updated...\")\n",
    "        waiter = cfn_client.get_waiter(\"stack_update_complete\")\n",
    "        waiter.wait(\n",
    "            StackName=stack_name,\n",
    "            WaiterConfig={\n",
    "                'Delay': 10,\n",
    "                'MaxAttempts': 150\n",
    "            }\n",
    "        )\n",
    "        print(\"CloudFormation stack updated.\")\n",
    "    except cfn_client.exceptions.ClientError as e:\n",
    "        print(e)\n",
    "\n",
    "print()\n",
    "for output in cfn_client.describe_stacks(StackName=stack_name).get('Stacks')[0].get('Outputs'):\n",
    "    print(f\"{output.get('OutputKey')}: {output.get('OutputValue')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4e996a-f8ea-485a-a9e8-c4dba5e7463d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Activity:</b> Navigate to the <a href=\"https://us-east-1.console.aws.amazon.com/cloudformation/home\">AWS CloudFormation mangement console</a> to check the status of your stack and see the individual resources being created (see the screenshot below). Also, you may review the template <code>batch-environment.yaml</code> to learn how we describe the resources for CloudFormation to create for us.\n",
    "</div>\n",
    "\n",
    "![](./images/cfn-console.png)\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Activity:</b> Assign the stack outputs to the related variables below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1942f272-3e48-4bbf-86ae-43ddd387926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bucket_name = \"\"  # FIXME\n",
    "batch_image_repository_uri = \"\"  # FIXME\n",
    "job_queue = \"\"  # FIXME\n",
    "job_definition = \"\"  # FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f4166f-4f3f-498b-a0ea-1593a5abb2e8",
   "metadata": {},
   "source": [
    "### Algorithm code and runtime\n",
    "\n",
    "Now that we have setup Batch to run jobs, we have to provide the algorithm code and describe the job runtime. For that purpose, we have to create a Docker image from which Batch will start a container to execute our workload. For this tutorial we are providing you with all the files, including the source code for the QMC example, the Dockerfile and the container entrypoint script so you don't have to code all up yourself. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Activity:</b> (Optional) To familiarize yourself with the provided code, you may review the directories <code>batch_container_image</code> and <code>afqmc</code>.\n",
    "</div>\n",
    "\n",
    "Now let's build and push our Docker image. Since we push it to a private repository, we must authenticate our local Docker client to our private ECR registry in our AWS account (learn more [here](https://docs.aws.amazon.com/AmazonECR/latest/userguide/registry_auth.html)), first. Next, we build the Docker image locally in our development envrionment. Finally, we push the image to the repository.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> The next cell takes about 3 minutes to complete.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af9fbed-9e82-4b2d-97d3-871cbac0dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "print(\"Authenticating Docker to your Amazon ECR private registry...\")\n",
    "os.system(f\"aws ecr get-login-password --region {WORKING_REGION} | docker login --username AWS --password-stdin {ACCOUNT_ID}.dkr.ecr.{WORKING_REGION}.amazonaws.com\")\n",
    "\n",
    "print(\"Building your Docker image locally...\")\n",
    "os.system(f\"docker build --quiet --platform linux/amd64 -f batch_container_image/Dockerfile -t {batch_image_repository_uri} .\")\n",
    "\n",
    "print(\"Pushing your Docker image to your ECR repository...\")\n",
    "os.system(f\"docker push --quiet {batch_image_repository_uri}\")\n",
    "\n",
    "print(\"All done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f20c0c-8181-4780-a6bb-8d7477a26c68",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Activity:</b> Navigate to the <a href=\"https://us-east-1.console.aws.amazon.com/ecr/private-registry/repositories\">Amazon ECR mangement console</a> and check to see that the image has been successfully pushed to the ECR repository. See the screenshot below.\n",
    "</div>\n",
    "\n",
    "![](images/ecr-console.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d885c2-0cf5-4da6-aa7d-71814e3c8514",
   "metadata": {},
   "source": [
    "### Submit a job to AWS Batch\n",
    "\n",
    "Now that we have created a compute environment for Batch and uploaded a container image defining our algorithm execution to ECR, it's time to submit our first QMC job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a251533-31d1-49fb-afad-1f2983d57b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = batch_client.submit_job(\n",
    "    jobName=\"lab-1-classical-small-scale\",\n",
    "    jobQueue=job_queue,\n",
    "    jobDefinition=job_definition,\n",
    "    containerOverrides={\n",
    "        \"environment\": [\n",
    "            {\"name\": \"JOB_ENTRY_POINT\", \"value\": \"run_classical_afqmc\"},\n",
    "            {\"name\": \"JOB_DTAU\", \"value\": \"0.005\"},\n",
    "            {\"name\": \"JOB_TIME_STEPS\", \"value\": \"200\"}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "jobId = response[\"jobId\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f865b25-eadc-4a3e-b365-b01090fad85a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>\n",
    "<ul>\n",
    "<li> The envrionment variable <b>JOB_ENTRY_POINT</b> that we specified during job submission is used to select our algorithm script.</li>\n",
    "<li>The container entrypoint script writes to a file in the local directory and uploads the results to our S3 bucket.</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Activity:</b> Navigate to the <a href=\"https://us-east-1.console.aws.amazon.com/batch/home\">AWS Batch mangement console</a> and find the job you submitted above. See the screenshot below.\n",
    "</div>\n",
    "\n",
    "![](images/batch-console.png)\n",
    "\n",
    "Your Batch job will undergo several lifecycle transitions indicated by follwoing states: `SUBMITTED`, `PENDING`, `RUNNABLE`, `STARTING`, `RUNNING`, `SUCCEEDED`.\n",
    "\n",
    "Learn more about Batch job states __[here](https://docs.aws.amazon.com/batch/latest/userguide/job_states.html)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21df0e96-1d07-4e57-abd5-5e9f8338ded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_client.describe_jobs(jobs=[jobId]).get(\"jobs\")[0].get(\"status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536e1c86-4adc-493e-b181-dd9fc0ac182d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Activity:</b> Once your job is in the <code>SUCCEEDED</code> state, navigate to the S3 management console and to locate the results file.\n",
    "</div>\n",
    "\n",
    "### Postprocessing of job results\n",
    "\n",
    "Next, we can download the job result and postprocess the data, locally.\n",
    "\n",
    "The total energy of AFQMC at every time step is evaluated by weight-averaging the local energy of every walker sample as\n",
    "$$\n",
    "E = \\sum_{l=1}^{N} w_l \\frac{\\langle \\Psi_T|H|\\phi_l\\rangle}{\\langle \\Psi_T|\\phi_l\\rangle},\n",
    "$$\n",
    "where $|\\Psi_T\\rangle$ is the trial wavefunction. The local energies and weights are separately saved into the S3 bucket. For our first, small-scale example, there is only one file we have to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594d1d7b-9761-4ef8-9ed1-e3fb0cd01444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "job = batch_client.describe_jobs(jobs=[jobId])[\"jobs\"][0]\n",
    "job_status = job[\"status\"]\n",
    "if job_status == \"SUCCEEDED\":\n",
    "    Path(\"results/lab-1\").mkdir(parents=True, exist_ok=True)\n",
    "    s3_client.download_file(\n",
    "        data_bucket_name,\n",
    "        f\"batch/{jobId}/results.json\",\n",
    "        f\"results/lab-1/result.json\")\n",
    "else:\n",
    "    print(f\"Your job is in status {job_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a588ce93-9b12-47c6-af2c-56882d40e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local postprocessing of results\n",
    "local_energies = []\n",
    "local_energies_real = []\n",
    "local_energies_imag = []\n",
    "weights = []\n",
    "\n",
    "with open(f\"results/lab-1/result.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "[local_energies_real.append(j) for j in data[\"local_energies_real\"]]\n",
    "[local_energies_imag.append(j) for j in data[\"local_energies_imag\"]]\n",
    "[weights.append(j) for j in data[\"weights\"]]\n",
    "\n",
    "for i, j in zip(local_energies_real, local_energies_imag):\n",
    "    local_energies.append([ii+1.j*jj for ii, jj in zip(i, j)])\n",
    "\n",
    "energies = np.real(np.average(local_energies, weights=weights, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94401eba-8478-4528-951a-42b3f7397abe",
   "metadata": {},
   "source": [
    "Let's plot the energies and compare to the reference value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c60310-52fc-40b7-b9bd-bd5740789f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(\n",
    "    0.005 * np.arange(200),\n",
    "    energies,\n",
    "    linestyle=\"dashed\",\n",
    "    marker=\".\",\n",
    "    color=\"tab:blue\",\n",
    "    label=\"classical\",\n",
    ")\n",
    "plt.axhline(-1.137117067345732, linestyle=\"dashed\", color=\"black\")\n",
    "plt.title(r\"Ground state estimation of H$_2$ using AFQMC\", fontsize=16)\n",
    "plt.legend(fontsize=14, loc=\"upper right\")\n",
    "plt.xlabel(r\"$\\tau$\", fontsize=14)\n",
    "plt.ylabel(\"Energy\", fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.tick_params(direction=\"in\", labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a985d76-502c-49bb-8b77-e4027928ea55",
   "metadata": {},
   "source": [
    "The results might be oscillating but that's ok. This is because we have only employed a small number of samples. In the next lab, we're going to scale this calculation up using Batch.\n",
    "\n",
    "## Run classical AFQMC at scale\n",
    "\n",
    "Now, we're going to scale up the previous example to a larger number of QMC walkers, by using the Batch feature called [array jobs](https://docs.aws.amazon.com/batch/latest/userguide/array_jobs.html). Each individual task only executes 2 walkers just like in the previous example, and by expanding out more tasks, we increase the total number of walkers in this simulation. At the end of this lab, you will see the results become much more accurate than above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fb9f94-e1e6-4f99-8939-674355e000a0",
   "metadata": {},
   "source": [
    "### Submit the job\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Activity:</b> Submit the job and check their status in the Batch management console. You will notice that the remains in the <code>PENDING</code> state while the child processes run.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> This job is an array job and we define the number of child processes using <code>arrayProperties={\"size\" : 200}</code>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7938db-27d3-4ea7-88c7-f6e5c0116c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = batch_client.submit_job(\n",
    "    jobName=\"lab-1-classical-large-scale\",\n",
    "    jobQueue=job_queue,\n",
    "    jobDefinition=job_definition,\n",
    "    containerOverrides={\n",
    "        \"environment\": [\n",
    "            {\"name\": \"JOB_ENTRY_POINT\", \"value\": \"run_classical_afqmc\"},\n",
    "            {\"name\": \"JOB_DTAU\", \"value\": \"0.005\"},\n",
    "            {\"name\": \"JOB_TIME_STEPS\", \"value\": \"600\"}\n",
    "        ]\n",
    "    },\n",
    "    arrayProperties={\"size\": 200}\n",
    ")\n",
    "\n",
    "jobId = response[\"jobId\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfaea67-9a2a-475e-b4ef-d813ffa9cbb5",
   "metadata": {},
   "source": [
    "Track the job status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6382a472-59b8-4085-bcc1-5bdc03ed0e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_client.describe_jobs(jobs=[jobId]).get(\"jobs\")[0].get(\"status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a587a3-79f0-475e-b37d-34fc0f1b8472",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Activity:</b> When the jobs complete use the <a href=\"https://us-east-1.console.aws.amazon.com/s3/home\"> Amazon S3 management console</a> to locate the output. Then use Boto3 to fetch and plot the results.\n",
    "</div>\n",
    "\n",
    "### Postprocessing of job results\n",
    "\n",
    "The processing of our job results is computationally more intensive this time. We have to download 200 files and average the energies over a much larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259fc8c1-af88-49ef-bf83-621acb5def66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "job = batch_client.describe_jobs(jobs=[jobId])[\"jobs\"][0]\n",
    "job_status = job[\"status\"]\n",
    "if job_status == \"SUCCEEDED\":\n",
    "    Path(\"results/lab-1\").mkdir(parents=True, exist_ok=True)\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    for i in range(200):\n",
    "        s3_client.download_file(\n",
    "            data_bucket_name, \n",
    "            f\"batch/{jobId}:{i}/results.json\",\n",
    "            f\"results/lab-1/result_{i}.json\"\n",
    "        )\n",
    "else:\n",
    "    print(f\"Your job is in status {job_status}\")\n",
    "\n",
    "\n",
    "\n",
    "local_energies_real = []\n",
    "local_energies_imag = []\n",
    "weights = []\n",
    "\n",
    "for i in range(200):\n",
    "    with open(f\"results/lab-1/result_{i}.json\", \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    [local_energies_real.append(j) for j in data[\"local_energies_real\"]]\n",
    "    [local_energies_imag.append(j) for j in data[\"local_energies_imag\"]]\n",
    "    [weights.append(j) for j in data[\"weights\"]]\n",
    "\n",
    "local_energies = [[ii+1.j*jj for ii, jj in zip(i, j)] for i, j in zip(local_energies_real, local_energies_imag)]   \n",
    "energies = np.real(np.average(local_energies, weights=weights, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e29db3-189a-43da-a023-a28630eea532",
   "metadata": {},
   "source": [
    "Let's plot the energies and compare to the reference value, again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d3a296-4da4-482e-b5e1-92ad31efff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(\n",
    "    0.005 * np.arange(600),\n",
    "    energies,\n",
    "    linestyle=\"dashed\",\n",
    "    marker=\".\",\n",
    "    color=\"tab:blue\",\n",
    "    label=\"classical\",\n",
    ")\n",
    "plt.axhline(-1.137117067345732, linestyle=\"dashed\", color=\"black\")\n",
    "plt.title(r\"Ground state estimation of H$_2$ using AFQMC\", fontsize=16)\n",
    "plt.legend(fontsize=14, loc=\"upper right\")\n",
    "plt.xlabel(r\"$\\tau$\", fontsize=14)\n",
    "plt.ylabel(\"Energy\", fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.tick_params(direction=\"in\", labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4523bf-9754-4b4a-b102-fc5e4ef44ddf",
   "metadata": {},
   "source": [
    "By comparing with the previous run, it's clear that the oscillations are much smaller and the results converge to the reference value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbeec2e-1759-438d-85d3-a94a9f6f909a",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>You reached the end of lab 1. Well done!</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_braket",
   "language": "python",
   "name": "conda_braket"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
