{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8d90c94-c526-4193-89c0-a972d0b0b72a",
   "metadata": {},
   "source": [
    "# Lab 3 - Build an run a fully integrated workflow for our QC-AFQMC experiment\n",
    "\n",
    "In the previous labs we have learned how to perform a classical and AFQMC simulation using AWS Batch, and a quantum AFQMC simulation with Amazon Braket and AWS Batch. For each case, we executed the indivudal steps in the workflow manually in the notebook. For the QC-AFQMC run, we first generated matchgate shadows using quantum computing resources in a hybrid job on Amazon Braket, we then performed a classical post-processing of the collected shadows on AWS Batch, and we finally collected the results of the parallel child jobs locally to reduce the data for comparison with the classical AFQMC run from the first lab.\n",
    "\n",
    "In this lab, we build a fully integrated workflow to execute the entire QC-AFQMC pipeline in one consistent and reproducible run, with the individual steps orchestrated automatically. For that purpose, we leverage another service, __[AWS Step Functions](https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html)__. \n",
    "\n",
    "With AWS Step Functions, you can create workflows, also called *state machines*, to build distributed applications, automate processes, orchestrate microservices, and create data and machine learning pipelines. Particularly interesting for our use case is the [integration of Step Functions with Batch](https://docs.aws.amazon.com/step-functions/latest/dg/connect-batch.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6b460f-eb34-4d77-a9cf-da2e5082fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "cfn_client = boto3.client('cloudformation')\n",
    "batch_client = boto3.client('batch')\n",
    "sts_client = boto3.client(\"sts\")\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sfn_client = boto3.client('stepfunctions')\n",
    "\n",
    "ACCOUNT_ID = sts_client.get_caller_identity().get(\"Account\")\n",
    "print(\"Account ID:\", ACCOUNT_ID)\n",
    "\n",
    "my_session = boto3.session.Session()\n",
    "WORKING_REGION = my_session.region_name\n",
    "print(\"Region:\", WORKING_REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46009834-24cf-4dd0-8f78-e8de9c4ea08a",
   "metadata": {},
   "source": [
    "## Create the resources for the workflow integration\n",
    "\n",
    "Let's build a Step Functions workflow to run the QC-AFQMC simulation from the second lab in a compute pipeline. In lab 2 we already learned how to run the individual steps:\n",
    "1. We generate matchgate shadows using quantum computing resource in an Amazon Braket hybrid job.\n",
    "2. We run large-scale classical post-processing on the collected shadows in an AWS Batch job.\n",
    "3. We have to collect the output files of the individual child jobs and reduce the samples containing the local energies.\n",
    "\n",
    "While, in lab 2, we have executed the third step locally in our notebook, we now want to run this in the clou, too. This data reduction step in our example doesn't require a lot of compute resources and runtime is expected to be only a few minutes. For that purpose, we use __[AWS Lambda](https://docs.aws.amazon.com/lambda/latest/dg/welcome.html)__ that allows us to easily run code without provisioning or managing compute infrastructure. We just have to provide our source code and Lambda takes care of rest.\n",
    "\n",
    "![](./images/workflow-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deb9344-0406-4e6f-82f4-4a45f00bcbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in cfn_client.describe_stacks(StackName='batch-environment').get('Stacks')[0].get('Outputs'):\n",
    "    print(f\"{output.get('OutputKey')}: {output.get('OutputValue')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af01e3e-4443-49ce-98d6-6d4c7ea1acfe",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Activity:</b> Assign the stack outputs to the related variables below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff1270-0037-431a-88f2-8e662329f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bucket_name = \"\"  # FIXME\n",
    "lambda_image_repository_uri = \"\"  # FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4b0081-5b39-45df-baca-1e48ee8b575f",
   "metadata": {},
   "source": [
    "### Provide the Lambda function code\n",
    "\n",
    "Similar to our Batch setup, we provide the code and runtime definition for our Lambda function with a Docker image which we build locally and upload to the dedicated image repository we have created earlier.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Activity:</b> (Optional) Check out the content in the <code>lambda_container_image</code> directory to familiarize yourself with the data reduction logic we run on AWS Lambda.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a635c23a-e213-4b6f-a34a-cd6efa00396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "print(\"Authenticating Docker to your Amazon ECR private registry...\")\n",
    "os.system(f\"aws ecr get-login-password --region {WORKING_REGION} | docker login --username AWS --password-stdin {ACCOUNT_ID}.dkr.ecr.{WORKING_REGION}.amazonaws.com\")\n",
    "\n",
    "print(\"Building your Docker image locally...\")\n",
    "os.system(f\"docker build --quiet --platform linux/amd64 -f lambda_container_image/Dockerfile -t {lambda_image_repository_uri} .\")\n",
    "\n",
    "print(\"Pushing your Docker image to your ECR repository...\")\n",
    "os.system(f\"docker push --quiet {lambda_image_repository_uri}\")\n",
    "\n",
    "print(\"All done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b501ff8-40c1-4f5c-8421-a9cd474e38b3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Activity:</b> Navigate to the <a href=\"https://us-east-1.console.aws.amazon.com/ecr/private-registry/repositories\">Amazon ECR mangement console</a> and check to see that the image has been successfully pushed to the repository. Note, that we are using a different repository than previously for our Batch image.\n",
    "</div>\n",
    "\n",
    "### Create the workflow\n",
    "\n",
    "Like in the first lab, we use CloudFormation to create the AWS resources for our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91df3fb-1b43-4ccd-afe4-8b976f60c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_queue_arn = batch_client.describe_job_queues().get('jobQueues')[0].get('jobQueueArn')\n",
    "batch_job_definition_arn = batch_client.describe_job_definitions(status='ACTIVE').get('jobDefinitions')[0].get('jobDefinitionArn')\n",
    "\n",
    "with open('hybrid-workflow.yaml', 'r') as file:\n",
    "    template_body = file.read()\n",
    "\n",
    "stack_name = 'braket-batch-workflow'\n",
    "\n",
    "try:\n",
    "    print(f\"Creating CloudFormation stack {stack_name}\")\n",
    "    cfn_client.create_stack(\n",
    "        StackName=stack_name,\n",
    "        TemplateBody=template_body,\n",
    "        Parameters=[\n",
    "            {'ParameterKey': 'BatchJobQueueArn', 'ParameterValue': batch_queue_arn},\n",
    "            {'ParameterKey': 'BatchJobDefinitionArn', 'ParameterValue': batch_job_definition_arn},\n",
    "            {'ParameterKey': 'DataBucket', 'ParameterValue': data_bucket_name},\n",
    "            {'ParameterKey': 'LambdaFunctionImageUri', 'ParameterValue': lambda_image_repository_uri},\n",
    "        ]\n",
    "    )\n",
    "    print(\"Waiting for CloudFormation stack to complete...\")\n",
    "    waiter = cfn_client.get_waiter(\"stack_create_complete\")\n",
    "    waiter.wait(\n",
    "        StackName=stack_name,\n",
    "        WaiterConfig={\n",
    "            'Delay': 10,\n",
    "            'MaxAttempts': 150\n",
    "        }\n",
    "    )\n",
    "    print(\"CloudFormation stack completed.\")\n",
    "except cfn_client.exceptions.AlreadyExistsException:\n",
    "    print(\"Stack already exists. Updating CloudFormation stack\")\n",
    "    try:\n",
    "        cfn_client.update_stack(\n",
    "            StackName=stack_name,\n",
    "            TemplateBody=template_body,\n",
    "            Parameters=[\n",
    "                {'ParameterKey': 'BatchJobQueueArn', 'ParameterValue': batch_queue_arn},\n",
    "                {'ParameterKey': 'BatchJobDefinitionArn', 'ParameterValue': batch_job_definition_arn},\n",
    "                {'ParameterKey': 'DataBucket', 'ParameterValue': data_bucket_name},\n",
    "                {'ParameterKey': 'LambdaFunctionImageUri', 'ParameterValue': lambda_image_repository_uri},\n",
    "            ]\n",
    "        )\n",
    "        print(\"Waiting for CloudFormation stack to be updated...\")\n",
    "        waiter = cfn_client.get_waiter(\"stack_update_complete\")\n",
    "        waiter.wait(\n",
    "            StackName=stack_name,\n",
    "            WaiterConfig={\n",
    "                'Delay': 10,\n",
    "                'MaxAttempts': 150\n",
    "            }\n",
    "        )\n",
    "        print(\"CloudFormation stack updated.\")\n",
    "    except cfn_client.exceptions.ClientError as e:\n",
    "        print(e)\n",
    "\n",
    "print()\n",
    "for output in cfn_client.describe_stacks(StackName=stack_name).get('Stacks')[0].get('Outputs'):\n",
    "    print(f\"{output.get('OutputKey')}: {output.get('OutputValue')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbe345d-9afb-4725-920e-04c736b50873",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Activity:</b> As previously, navigate to the <a href=\"https://us-east-1.console.aws.amazon.com/cloudformation/home\">AWS CloudFormation mangement console</a> to check the status of your stack and see the individual resources being created. Again, you may review the template <code>hybrid-workflow.yaml</code> to learn how we describe the resources for CloudFormation to create for us.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Activity:</b> Navigate to the <a href=\"https://us-east-1.console.aws.amazon.com/states/home\">AWS Step Functions mangement console</a> to check the definition of your workflow. See the screenshot below.\n",
    "</div>\n",
    "\n",
    "![](./images/stepfunctions-console.png)\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Activity:</b> Assign the stack outputs to the related variables below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9020cf3-a789-483b-819e-2aee3c034325",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_machine_arn = \"\"  # FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400d8828-42b9-4da0-90a2-90a5bfbe4efb",
   "metadata": {},
   "source": [
    "### Provide the Braket hybrid jobs code\n",
    "\n",
    "In the second lab, we have uploaded the code to generate matchgate shadows with quantum computing resources on the fly, when we create the hybrid job on Amazon Braket. This time, we have to upload a tarball of the code which we have referenced in our workflow definition (see line 46 in `hybrid-workflow.yaml`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af314c4-303a-48d8-ab29-5feadaf088d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "abs_path = Path(\"afqmc\").resolve(strict=True)\n",
    "\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    filename = \"afqmc.tar.gz\"\n",
    "    filepath = f\"{temp_dir}/afqmc.tar.gz\"\n",
    "    print(f\"Tarring up source code: {filepath}\")\n",
    "    with tarfile.open(filepath, \"w:gz\", dereference=True) as tar:\n",
    "        tar.add(abs_path, arcname=abs_path.name)\n",
    "    print(f\"Uploading tarball to S3: {data_bucket_name}/{filename}\") \n",
    "    s3_client.upload_file(filepath, data_bucket_name, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5456bb-6505-4132-9d48-a56f6394611e",
   "metadata": {},
   "source": [
    "## Execute the workflow described in Step Functions\n",
    "\n",
    "Now we're set to start the execution of our workload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875b3c8b-8f95-411c-ae15-c98502e4780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sfn_client.start_execution(stateMachineArn=state_machine_arn)\n",
    "\n",
    "execution_arn = response.get('executionArn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc7da03-18e1-47f0-b1e7-95e19f8be640",
   "metadata": {},
   "source": [
    "You can check the status of the workflow execution in the Step Functions management console or programmatically.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Activity:</b> Navigate to the <a href=\"https://us-east-1.console.aws.amazon.com/states/home\">AWS Step Functions mangement console</a> and monitor the workflow execution. See the screenshot below.\n",
    "</div>\n",
    "\n",
    "![](./images/workflow-execution.png)\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Activity:</b> Wait until the workflow execution completes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e088a-e45d-4946-9689-95af32b34b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfn_client.describe_execution(executionArn=execution_arn).get('status')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281d4b25-0e33-470c-8c77-cfb1c1c309eb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> It will take about 45 minutes for the entire workflow to complete. You can leave it running and come back later to retrieve the results.\n",
    "</div>\n",
    "\n",
    "## Retrieve results\n",
    "\n",
    "Loading the results of our QC-AFQMC run is trivial this time. We only have to retrieve one single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9beb0c5-d126-4211-9f28-96e3f228ee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "workflow_execution_status = sfn_client.describe_execution(executionArn=execution_arn).get('status')\n",
    "if workflow_execution_status == \"SUCCEEDED\":\n",
    "    Path(\"results/lab-3\").mkdir(parents=True, exist_ok=True)\n",
    "    s3_client.download_file(\n",
    "        data_bucket_name,\n",
    "        f\"lambda/final_result.json\",\n",
    "        f\"results/lab-3/result.json\")\n",
    "else:\n",
    "    print(f\"Your job is in status {workflow_execution_status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b8b142-fa5c-46ea-aca3-2a1ac52a0a65",
   "metadata": {},
   "source": [
    "Let's plot the energies and compare to results from the classical AF-QMC simulation and the reference value, again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d45c4bc-f547-4a0c-9dfa-de4f64798399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "%matplotlib inline\n",
    "\n",
    "with open(f\"results/lab-3/result.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "qc_energies = data[\"energies\"]\n",
    "\n",
    "local_energies_real = []\n",
    "local_energies_imag = []\n",
    "weights = []\n",
    "\n",
    "for i in range(200):\n",
    "    with open(f\"results/lab-1/result_{i}.json\", \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    [local_energies_real.append(j) for j in data[\"local_energies_real\"]]\n",
    "    [local_energies_imag.append(j) for j in data[\"local_energies_imag\"]]\n",
    "    [weights.append(j) for j in data[\"weights\"]]\n",
    "\n",
    "local_energies = [[ii+1.j*jj for ii, jj in zip(i, j)] for i, j in zip(local_energies_real, local_energies_imag)]   \n",
    "classical_energies = np.real(np.average(local_energies, weights=weights, axis=0))\n",
    "\n",
    "\n",
    "plt.plot(\n",
    "    0.005 * np.arange(600),\n",
    "    classical_energies,\n",
    "    linestyle=\"dashed\",\n",
    "    marker=\".\",\n",
    "    color=\"tab:blue\",\n",
    "    label=\"classical\",\n",
    ")\n",
    "plt.plot(\n",
    "    0.005 * np.arange(200),\n",
    "    qc_energies,\n",
    "    linestyle=\"dashed\",\n",
    "    marker=\".\",\n",
    "    color=\"tab:orange\",\n",
    "    label=\"quantum\",\n",
    ")\n",
    "plt.axhline(-1.137117067345732, linestyle=\"dashed\", color=\"black\")\n",
    "plt.title(r\"Ground state estimation of H$_2$ using AFQMC\", fontsize=16)\n",
    "plt.legend(fontsize=14, loc=\"upper right\")\n",
    "plt.xlabel(r\"$\\tau$\", fontsize=14)\n",
    "plt.ylabel(\"Energy\", fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.tick_params(direction=\"in\", labelsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e21bf1-8201-49a0-9def-fe2fce4f6695",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>You reached the end of lab 3 and our tutorial. Well done!</b>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Activity: Tell us how we did today.</b>\n",
    "\n",
    "Please take <a href=\"https://pulse.aws/promotion/F9HYM7N0\">this survey</a> and help us improve for similar tutorials at future events.\n",
    "\n",
    "<b>In return for your participation you'll receive a promotion code with which you can redeem credits worth $20 in your AWS account.</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_braket",
   "language": "python",
   "name": "conda_braket"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
